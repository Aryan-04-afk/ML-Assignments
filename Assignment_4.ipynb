{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "Q1. Write a Python program to scrape all available books from the website\n",
        "(https://books.toscrape.com/) Books to Scrape – a live site built for practicing scraping (safe,\n",
        "legal, no anti-bot). For each book, extract the following details:\n",
        "1. Title\n",
        "2. Price\n",
        "3. Availability (In stock / Out of stock)\n",
        "4. Star Rating (One, Two, Three, Four, Five)\n",
        "Store the scraped results into a Pandas DataFrame and export them to a CSV file named\n",
        "books.csv.\n",
        "(Note: Use the requests library to fetch the HTML page. Use BeautifulSoup to parse and extract\n",
        "book details and handle pagination so that books from all pages are scraped)"
      ],
      "metadata": {
        "id": "jTHw3GqEfWrw"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 17
        },
        "id": "ctyQ2QtSeRaR",
        "outputId": "502a95a7-353c-4703-8722-abd5ca62fd9a"
      },
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.Javascript object>"
            ],
            "application/javascript": [
              "\n",
              "    async function download(id, filename, size) {\n",
              "      if (!google.colab.kernel.accessAllowed) {\n",
              "        return;\n",
              "      }\n",
              "      const div = document.createElement('div');\n",
              "      const label = document.createElement('label');\n",
              "      label.textContent = `Downloading \"${filename}\": `;\n",
              "      div.appendChild(label);\n",
              "      const progress = document.createElement('progress');\n",
              "      progress.max = size;\n",
              "      div.appendChild(progress);\n",
              "      document.body.appendChild(div);\n",
              "\n",
              "      const buffers = [];\n",
              "      let downloaded = 0;\n",
              "\n",
              "      const channel = await google.colab.kernel.comms.open(id);\n",
              "      // Send a message to notify the kernel that we're ready.\n",
              "      channel.send({})\n",
              "\n",
              "      for await (const message of channel.messages) {\n",
              "        // Send a message to notify the kernel that we're ready.\n",
              "        channel.send({})\n",
              "        if (message.buffers) {\n",
              "          for (const buffer of message.buffers) {\n",
              "            buffers.push(buffer);\n",
              "            downloaded += buffer.byteLength;\n",
              "            progress.value = downloaded;\n",
              "          }\n",
              "        }\n",
              "      }\n",
              "      const blob = new Blob(buffers, {type: 'application/binary'});\n",
              "      const a = document.createElement('a');\n",
              "      a.href = window.URL.createObjectURL(blob);\n",
              "      a.download = filename;\n",
              "      div.appendChild(a);\n",
              "      a.click();\n",
              "      div.remove();\n",
              "    }\n",
              "  "
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.Javascript object>"
            ],
            "application/javascript": [
              "download(\"download_195e7092-540e-41a4-8898-6f89b5da6789\", \"books.csv\", 62219)"
            ]
          },
          "metadata": {}
        }
      ],
      "source": [
        "import requests\n",
        "from bs4 import BeautifulSoup\n",
        "import pandas as pd\n",
        "from google.colab import files\n",
        "\n",
        "BASE_URL = \"https://books.toscrape.com/\"\n",
        "books = []\n",
        "\n",
        "def get_star_rating(tag):\n",
        "    classes = tag.get(\"class\", [])\n",
        "    ratings = [\"One\", \"Two\", \"Three\", \"Four\", \"Five\"]\n",
        "    for r in ratings:\n",
        "        if r in classes:\n",
        "            return r\n",
        "    return \"Unknown\"\n",
        "\n",
        "def scrape_page(url):\n",
        "    response = requests.get(url)\n",
        "    soup = BeautifulSoup(response.content, \"html.parser\")\n",
        "    articles = soup.find_all(\"article\", class_=\"product_pod\")\n",
        "    for article in articles:\n",
        "        title = article.h3.a[\"title\"]\n",
        "        price = article.find(\"p\", class_=\"price_color\").text.strip()\n",
        "        availability = article.find(\"p\", class_=\"instock availability\").text.strip()\n",
        "        rating = get_star_rating(article.find(\"p\", class_=\"star-rating\"))\n",
        "        books.append({\n",
        "            \"Title\": title,\n",
        "            \"Price\": price,\n",
        "            \"Availability\": availability,\n",
        "            \"Star Rating\": rating\n",
        "        })\n",
        "    next_btn = soup.find(\"li\", class_=\"next\")\n",
        "    if next_btn:\n",
        "        next_href = next_btn.a[\"href\"]\n",
        "        if \"catalogue/\" not in url and \"catalogue/\" not in next_href:\n",
        "            next_url = BASE_URL + \"catalogue/\" + next_href\n",
        "        else:\n",
        "            base = url.rsplit(\"/\", 1)[0]\n",
        "            next_url = base + \"/\" + next_href\n",
        "        scrape_page(next_url)\n",
        "\n",
        "scrape_page(BASE_URL + \"catalogue/page-1.html\")\n",
        "df = pd.DataFrame(books)\n",
        "df.to_csv(\"books.csv\", index=False)\n",
        "\n",
        "files.download(\"books.csv\")\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install selenium\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 725
        },
        "id": "R4held8-jIVI",
        "outputId": "d31eb08f-0926-4ed8-dc68-e98d70b6acfb"
      },
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting selenium\n",
            "  Downloading selenium-4.35.0-py3-none-any.whl.metadata (7.4 kB)\n",
            "Requirement already satisfied: urllib3<3.0,>=2.5.0 in /usr/local/lib/python3.12/dist-packages (from urllib3[socks]<3.0,>=2.5.0->selenium) (2.5.0)\n",
            "Collecting trio~=0.30.0 (from selenium)\n",
            "  Downloading trio-0.30.0-py3-none-any.whl.metadata (8.5 kB)\n",
            "Collecting trio-websocket~=0.12.2 (from selenium)\n",
            "  Downloading trio_websocket-0.12.2-py3-none-any.whl.metadata (5.1 kB)\n",
            "Requirement already satisfied: certifi>=2025.6.15 in /usr/local/lib/python3.12/dist-packages (from selenium) (2025.8.3)\n",
            "Collecting typing_extensions~=4.14.0 (from selenium)\n",
            "  Downloading typing_extensions-4.14.1-py3-none-any.whl.metadata (3.0 kB)\n",
            "Requirement already satisfied: websocket-client~=1.8.0 in /usr/local/lib/python3.12/dist-packages (from selenium) (1.8.0)\n",
            "Requirement already satisfied: attrs>=23.2.0 in /usr/local/lib/python3.12/dist-packages (from trio~=0.30.0->selenium) (25.3.0)\n",
            "Requirement already satisfied: sortedcontainers in /usr/local/lib/python3.12/dist-packages (from trio~=0.30.0->selenium) (2.4.0)\n",
            "Requirement already satisfied: idna in /usr/local/lib/python3.12/dist-packages (from trio~=0.30.0->selenium) (3.10)\n",
            "Collecting outcome (from trio~=0.30.0->selenium)\n",
            "  Downloading outcome-1.3.0.post0-py2.py3-none-any.whl.metadata (2.6 kB)\n",
            "Requirement already satisfied: sniffio>=1.3.0 in /usr/local/lib/python3.12/dist-packages (from trio~=0.30.0->selenium) (1.3.1)\n",
            "Collecting wsproto>=0.14 (from trio-websocket~=0.12.2->selenium)\n",
            "  Downloading wsproto-1.2.0-py3-none-any.whl.metadata (5.6 kB)\n",
            "Requirement already satisfied: pysocks!=1.5.7,<2.0,>=1.5.6 in /usr/local/lib/python3.12/dist-packages (from urllib3[socks]<3.0,>=2.5.0->selenium) (1.7.1)\n",
            "Requirement already satisfied: h11<1,>=0.9.0 in /usr/local/lib/python3.12/dist-packages (from wsproto>=0.14->trio-websocket~=0.12.2->selenium) (0.16.0)\n",
            "Downloading selenium-4.35.0-py3-none-any.whl (9.6 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m9.6/9.6 MB\u001b[0m \u001b[31m92.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading trio-0.30.0-py3-none-any.whl (499 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m499.2/499.2 kB\u001b[0m \u001b[31m37.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading trio_websocket-0.12.2-py3-none-any.whl (21 kB)\n",
            "Downloading typing_extensions-4.14.1-py3-none-any.whl (43 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m43.9/43.9 kB\u001b[0m \u001b[31m3.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading outcome-1.3.0.post0-py2.py3-none-any.whl (10 kB)\n",
            "Downloading wsproto-1.2.0-py3-none-any.whl (24 kB)\n",
            "Installing collected packages: wsproto, typing_extensions, outcome, trio, trio-websocket, selenium\n",
            "  Attempting uninstall: typing_extensions\n",
            "    Found existing installation: typing_extensions 4.15.0\n",
            "    Uninstalling typing_extensions-4.15.0:\n",
            "      Successfully uninstalled typing_extensions-4.15.0\n",
            "Successfully installed outcome-1.3.0.post0 selenium-4.35.0 trio-0.30.0 trio-websocket-0.12.2 typing_extensions-4.14.1 wsproto-1.2.0\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.colab-display-data+json": {
              "pip_warning": {
                "packages": [
                  "typing_extensions"
                ]
              },
              "id": "643646382cf647b180631b5331476a00"
            }
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Q2. Write a Python program to scrape the IMDB Top 250 Movies list\n",
        "(https://www.imdb.com/chart/top/) . For each movie, extract the following details:\n",
        "1. Rank (1–250)\n",
        "2. Movie Title\n",
        "3. Year of Release\n",
        "4. IMDB Rating\n",
        "Store the results in a Pandas DataFrame and export it to a CSV file named imdb_top250.csv.\n",
        "(Note: Use Selenium/Playwright to scrape the required details from this website)"
      ],
      "metadata": {
        "id": "8aDol0Y6hEDq"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from selenium import webdriver\n",
        "from selenium.webdriver.chrome.service import Service\n",
        "from selenium.webdriver.common.by import By\n",
        "from selenium.webdriver.chrome.options import Options\n",
        "import pandas as pd\n",
        "import time\n",
        "from google.colab import files\n",
        "\n",
        "options = Options()\n",
        "options.add_argument(\"--headless\")\n",
        "driver = webdriver.Chrome(options=options)\n",
        "\n",
        "url = \"https://www.imdb.com/chart/top/\"\n",
        "driver.get(url)\n",
        "time.sleep(3)\n",
        "\n",
        "movies = []\n",
        "rows = driver.find_elements(By.CSS_SELECTOR, \"tbody.lister-list tr\")\n",
        "\n",
        "for i, row in enumerate(rows, start=1):\n",
        "    title_column = row.find_element(By.CSS_SELECTOR, \"td.titleColumn\")\n",
        "    title = title_column.find_element(By.TAG_NAME, \"a\").text\n",
        "    year = title_column.find_element(By.CLASS_NAME, \"secondaryInfo\").text.strip(\"()\")\n",
        "    rating = row.find_element(By.CSS_SELECTOR, \"td.imdbRating strong\").text\n",
        "    movies.append({\n",
        "        \"Rank\": i,\n",
        "        \"Movie Title\": title,\n",
        "        \"Year of Release\": year,\n",
        "        \"IMDB Rating\": rating\n",
        "    })\n",
        "\n",
        "driver.quit()\n",
        "df = pd.DataFrame(movies)\n",
        "df.to_csv(\"imdb_top250.csv\", index=False)\n",
        "files.download(\"imdb_top250.csv\")"
      ],
      "metadata": {
        "id": "sKL4_htnhDwU"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Q3. Write a Python program to scrape the weather information for top world cities from the\n",
        "given website (https://www.timeanddate.com/weather/) . For each city, extract the following\n",
        "details:\n",
        "1. City Name\n",
        "2. Temperature\n",
        "3. Weather Condition (e.g., Clear, Cloudy, Rainy, etc.)\n",
        "Store the results in a Pandas DataFrame and export it to a CSV file named weather.csv."
      ],
      "metadata": {
        "id": "oWHbPrn4hWNe"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import requests\n",
        "from bs4 import BeautifulSoup\n",
        "import pandas as pd\n",
        "\n",
        "cities = [\n",
        "    (\"London\", \"uk/london\"),\n",
        "    (\"New York\", \"usa/new-york\"),\n",
        "    (\"Tokyo\", \"japan/tokyo\"),\n",
        "    (\"Paris\", \"france/paris\"),\n",
        "    (\"Sydney\", \"australia/sydney\")\n",
        "]\n",
        "\n",
        "base_url = \"https://www.timeanddate.com/weather/\"\n",
        "headers = {\"User-Agent\": \"Mozilla/5.0\"}\n",
        "\n",
        "records = []\n",
        "\n",
        "for city_name, path in cities:\n",
        "    url = base_url + path\n",
        "    try:\n",
        "        resp = requests.get(url, headers=headers, timeout=10)\n",
        "        resp.raise_for_status()\n",
        "        soup = BeautifulSoup(resp.content, \"html.parser\")\n",
        "\n",
        "        qlook = soup.select_one(\"#qlook\")\n",
        "        if qlook:\n",
        "            temp_elem = qlook.find(\"div\", class_=\"h2\")\n",
        "            cond_elem = qlook.find(\"p\")\n",
        "\n",
        "            temp = temp_elem.text.strip() if temp_elem else \"N/A\"\n",
        "            cond = cond_elem.text.strip() if cond_elem else \"N/A\"\n",
        "\n",
        "            records.append({\n",
        "                \"City Name\": city_name,\n",
        "                \"Temperature\": temp,\n",
        "                \"Weather Condition\": cond\n",
        "            })\n",
        "        else:\n",
        "            records.append({\n",
        "                \"City Name\": city_name,\n",
        "                \"Temperature\": \"N/A\",\n",
        "                \"Weather Condition\": \"N/A\"\n",
        "            })\n",
        "\n",
        "    except Exception as e:\n",
        "        records.append({\n",
        "            \"City Name\": city_name,\n",
        "            \"Temperature\": \"Error\",\n",
        "            \"Weather Condition\": str(e)\n",
        "        })\n",
        "\n",
        "df = pd.DataFrame(records)\n",
        "df.to_csv(\"weather.csv\", index=False)\n",
        "print(\"Weather data saved to weather.csv\")\n",
        "files.download('weather.csv')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "id": "E2HeiJmDhdZ8",
        "outputId": "f461f5c3-2a23-451a-ff12-1141da079fbd"
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Weather data saved to weather.csv\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.Javascript object>"
            ],
            "application/javascript": [
              "\n",
              "    async function download(id, filename, size) {\n",
              "      if (!google.colab.kernel.accessAllowed) {\n",
              "        return;\n",
              "      }\n",
              "      const div = document.createElement('div');\n",
              "      const label = document.createElement('label');\n",
              "      label.textContent = `Downloading \"${filename}\": `;\n",
              "      div.appendChild(label);\n",
              "      const progress = document.createElement('progress');\n",
              "      progress.max = size;\n",
              "      div.appendChild(progress);\n",
              "      document.body.appendChild(div);\n",
              "\n",
              "      const buffers = [];\n",
              "      let downloaded = 0;\n",
              "\n",
              "      const channel = await google.colab.kernel.comms.open(id);\n",
              "      // Send a message to notify the kernel that we're ready.\n",
              "      channel.send({})\n",
              "\n",
              "      for await (const message of channel.messages) {\n",
              "        // Send a message to notify the kernel that we're ready.\n",
              "        channel.send({})\n",
              "        if (message.buffers) {\n",
              "          for (const buffer of message.buffers) {\n",
              "            buffers.push(buffer);\n",
              "            downloaded += buffer.byteLength;\n",
              "            progress.value = downloaded;\n",
              "          }\n",
              "        }\n",
              "      }\n",
              "      const blob = new Blob(buffers, {type: 'application/binary'});\n",
              "      const a = document.createElement('a');\n",
              "      a.href = window.URL.createObjectURL(blob);\n",
              "      a.download = filename;\n",
              "      div.appendChild(a);\n",
              "      a.click();\n",
              "      div.remove();\n",
              "    }\n",
              "  "
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.Javascript object>"
            ],
            "application/javascript": [
              "download(\"download_95687fde-9132-4c29-9749-584f963f11e1\", \"weather.csv\", 180)"
            ]
          },
          "metadata": {}
        }
      ]
    }
  ]
}